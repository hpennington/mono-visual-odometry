{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Odometry pipeline\n",
    "A naive visual odometry implementation with OpenCV (for now), NumPy & Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch data\n",
    "!bash ../fetch_data.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run with KITTI\n",
    "Make sure to download the KITTI odometry dataset and set KITTI=True\n",
    "\n",
    "### Run with ad-hoc video\n",
    "Set KITTI variable to False and input variable to the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "# Handrolled\n",
    "from utils import display_mat, draw_points\n",
    "from ransac import ransac\n",
    "from features import extract_features, bruteforce_match\n",
    "from geometry import triangulate, fundamental_to_essential, \\\n",
    "    extract_pose, normalize, FundamentalMatrixModel, integrate_pose, calculate_projection, create_normalization_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "- KITTI: Defines if the KITTI dataset should be used (requires download from kaggle or the KITTI website)\n",
    "- input: If KITTI isn't used, this is the file to load for doing VO.\n",
    "- max_corners: Maximum keypoints returned from goodFeatureToTrack.\n",
    "- kernel_size: Size of the kernel to search for features with.\n",
    "- quality: Corner detector \"quality\".\n",
    "- ransac_minsamples: The number of samples used by RANSAC\n",
    "- ransac_max_trial: maximum iteraions of the RANSAC search.\n",
    "- ransac_residual_threshold: The residual threshold for ransac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script parameters\n",
    "KITTI = True\n",
    "KITTI_DATA_DIRECTORY = '/Users/haydenpennington/dev/data/kitti/'\n",
    "input = '../videos/test_countryroad.mp4'\n",
    "# input = 0\n",
    "im_size = (640, 480)\n",
    "\n",
    "# ORB Detector parameters\n",
    "max_corners = 10000\n",
    "kernel_size = 3\n",
    "quality = 0.00001\n",
    "min_distance = 6\n",
    "\n",
    "# Ransac parameters\n",
    "ransac_minsamples = 11\n",
    "ransac_max_trials = 100\n",
    "ransac_residual_threshold = 0.001\n",
    "\n",
    "# Pose extratction translation scaling\n",
    "tscale = 1.0\n",
    "\n",
    "# Point cloud clustering\n",
    "n_points = 3\n",
    "dbscan_eps = tscale * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "seq = None\n",
    "mono_folder = None\n",
    "data_folder = None\n",
    "train_image_names = None\n",
    "train_labels = None\n",
    "\n",
    "if KITTI == True:\n",
    "    seq = '00'  \n",
    "    mono_folder = 'image_2'\n",
    "    data_root = KITTI_DATA_DIRECTORY\n",
    "    data_folder = data_root + 'sequences/' + seq + '/' + mono_folder + '/'\n",
    "    train_image_names = sorted([f for f in listdir(data_folder) if isfile(join(data_folder, f))])\n",
    "    \n",
    "    with open(data_root + 'poses/' + seq + '.txt', 'r') as f:\n",
    "        train_labels = [x.split() for x in f.readlines()]\n",
    "        train_labels = np.array([[float(x) for x in y] for y in train_labels])\n",
    "        train_labels = train_labels.reshape(-1, 3, 4)\n",
    "\n",
    "\n",
    "cap = None\n",
    "if KITTI == False:\n",
    "    cap = cv2.VideoCapture(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(i, im_size):\n",
    "    if KITTI == True:\n",
    "        im = cv2.imread(data_folder + train_image_names[i])\n",
    "        im_bw = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "        im_resized = cv2.resize(im_bw, im_size)\n",
    "        return True, im, im_resized\n",
    "    else:\n",
    "        ret, im = cap.read()\n",
    "        im_bw = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "        im_resized = cv2.resize(im_bw, im_size)\n",
    "        return ret, im, im_resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_frames(T, H, W, kps2, im1, im2):    \n",
    "\n",
    "    p1, st, err = cv2.calcOpticalFlowPyrLK(im1, im2, kps2, None)\n",
    "\n",
    "    pairs = np.asarray(list(zip(kps2[st == 1], p1[st == 1])))\n",
    "    # pairs = []\n",
    "    norm_pairs = []\n",
    "\n",
    "    # for m,n in matches:\n",
    "    #     # Check Lowe's ratio\n",
    "    #     if m.distance < 0.75 * n.distance:\n",
    "    #         pt1 = np.asarray(kps1[m.queryIdx].pt)\n",
    "    #         pt2 = np.asarray(kps2[m.trainIdx].pt)\n",
    "    #         pairs.append((pt1, pt2))\n",
    "\n",
    "    # pairs = np.asarray(pairs)\n",
    "    norm_pairs = np.zeros_like(pairs)\n",
    "    norm_x = normalize(T, pairs[:, 0])\n",
    "    norm_y = normalize(T, pairs[:, 1])\n",
    "    norm_pairs[:, 0] = norm_x\n",
    "    norm_pairs[:, 1] = norm_y\n",
    "\n",
    "    # Filter with ransac\n",
    "    if norm_pairs[:, 0].shape[0] >= ransac_minsamples and norm_pairs[:, 1].shape[0] >= ransac_minsamples:\n",
    "        F, inliers = ransac(\n",
    "            FundamentalMatrixModel(),\n",
    "            norm_pairs,\n",
    "            min_samples=ransac_minsamples,\n",
    "            residual_threshold=ransac_residual_threshold,\n",
    "            max_trials=ransac_max_trials\n",
    "        )\n",
    "\n",
    "        return norm_pairs[inliers], pairs[inliers], F\n",
    "\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last = None\n",
    "last_kps = None\n",
    "last_descriptors = None\n",
    "last_proj = None\n",
    "point_cloud = None\n",
    "\n",
    "pose_abs = np.eye(4)\n",
    "t_abs_all = []\n",
    "t_abs_gt = []\n",
    "\n",
    "i = 0\n",
    "\n",
    "while (KITTI == True and i < len(train_image_names)) or (cap is not None and cap.isOpened()):\n",
    "    t0 = time.time()\n",
    "    ret, im_original, im = get_image(i, im_size)\n",
    "    \n",
    "    label = None\n",
    "    \n",
    "    if KITTI:\n",
    "        label = train_labels[i]\n",
    "    \n",
    "    if ret:\n",
    "        \n",
    "        multiplier_x = im_original.shape[1] / im.shape[1]\n",
    "        multiplier_y = im_original.shape[0] / im.shape[0]\n",
    "        \n",
    "        # Create the transformation matrix for normalization\n",
    "        H, W = im.shape\n",
    "        T = create_normalization_matrix(H, W)\n",
    "\n",
    "        if last is not None:\n",
    "            im1 = im\n",
    "            im2 = last\n",
    "\n",
    "            # Extract features with OpenCV\n",
    "            kps = extract_features(im1, max_corners, quality, kernel_size, min_distance)\n",
    "            H, W = im1.shape\n",
    "\n",
    "            # Use OpenCV's Brute-force matcher to find feature matches between images\n",
    "            # and compute the filtered pairs of images \n",
    "            # as well as the fundamental matrix F\n",
    "            result = match_frames(T, H, W, last_kps, im1, im2)\n",
    "\n",
    "            if result is not None:\n",
    "\n",
    "                norm_pairs, feature_pairs, F = result\n",
    "\n",
    "                if norm_pairs.shape[0] > 1:\n",
    "\n",
    "                    # Compute the essential matrix from the fundamental matrix\n",
    "                    E = fundamental_to_essential(F)\n",
    "                    \n",
    "                    # Compute the rotation matrix and translation vector between the two frames\n",
    "                    R, t = extract_pose(E, norm_pairs[0, 0], norm_pairs[0, 1])\n",
    "                    \n",
    "                    # Integrate the rotation, and translation matrix/vector with the previos pose \n",
    "                    # to compute the current pose.\n",
    "                    pose_abs = integrate_pose(pose_abs, R, t)\n",
    "                    \n",
    "                    # Get the translation of the camera from 4d -> 3d coordinates.\n",
    "                    t_abs = pose_abs[:3, 3] / pose_abs[3, 3]\n",
    "                    t_abs_all.append(t_abs)\n",
    "                    \n",
    "                    if KITTI == True:\n",
    "                        t_abs_gt.append(label[:3, 3])\n",
    "                    \n",
    "                    Rt = np.eye(4)\n",
    "                    proj1 = Rt\n",
    "\n",
    "                    if last_proj is not None:\n",
    "                        proj1 = calculate_projection(R, t, last_proj)\n",
    "                        kps1 = norm_pairs[:, 0]\n",
    "                        kps2 = norm_pairs[:, 1]\n",
    "                        \n",
    "                        # Triangulate 4d points\n",
    "                        points4d = triangulate(proj1[:3, :4], last_proj[:3, :4], kps1, kps2, R, t)\n",
    "                        # 4d homogeneous -> 3d\n",
    "                        points3d = (points4d / points4d[:, 3:])[:, :3]\n",
    "\n",
    "                        if point_cloud is None:\n",
    "                            point_cloud = points3d.reshape(-1, 3)\n",
    "                        else:\n",
    "                            point_cloud = np.concatenate([point_cloud, points3d.reshape(-1, 3)]).reshape(-1, 3)\n",
    "                    \n",
    "                    last_proj = proj1\n",
    "                    \n",
    "                else:\n",
    "                    print(\"norm_pairs.shape[0] < 1\")\n",
    "\n",
    "               \n",
    "\n",
    "                draw_points(im_original, feature_pairs, multiplier_x, multiplier_y)\n",
    "        \n",
    "        last = im\n",
    "\n",
    "        if last_kps is None:\n",
    "            last_kps = extract_features(im, max_corners, quality, kernel_size, min_distance)\n",
    "        else:\n",
    "            last_kps = kps\n",
    "\n",
    "        cv2.imshow('im', im_original)\n",
    "        key = cv2.waitKey(1)\n",
    "        if key & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "        if (i > 99 and i % 100 == 0) or (i > 0 and i < 11 and i % 10 == 0):\n",
    "            display_mat(cv2.cvtColor(im_original, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        t1 = time.time()\n",
    "        t_delta = (t1 - t0)\n",
    "        # print (1 / t_delta)\n",
    "        i += 1\n",
    "\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(4)\n",
    "if KITTI == False:\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the point cloud for viewing in open3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_abs_all = np.array(t_abs_all).reshape(-1, 3)\n",
    "\n",
    "if point_cloud is not None:\n",
    "    \n",
    "    import open3d as o3d\n",
    "\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.transform([\n",
    "        [-1, 0,  0, 0],\n",
    "        [ 0, 1,  0, 0],\n",
    "        [ 0, 0, 1, 0],\n",
    "        [ 0, 0,  0, 1]\n",
    "    ])\n",
    "    pcd.points = o3d.utility.Vector3dVector(point_cloud)\n",
    "    ret = np.array(pcd.cluster_dbscan(eps=dbscan_eps, min_points=n_points))\n",
    "    all_points = np.concatenate([point_cloud[ret > -1], t_abs_all])\n",
    "    \n",
    "    pcd.points = o3d.utility.Vector3dVector(all_points)\n",
    "    \n",
    "    colors = np.full(all_points.shape, fill_value=(255, 0, 0))\n",
    "    colors[point_cloud[ret>-1].shape[0]:point_cloud[ret>-1].shape[0]+t_abs_all.shape[0]] = [0, 255, 0]\n",
    "\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "\n",
    "\n",
    "    o3d.io.write_point_cloud(\"../pointcloud_clustered.pcd\", pcd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the 3d trajectory of the camera\n",
    "### Red vs Green\n",
    "Red is the ground truth pose and green is the predicted pose of the camera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "plt.ion()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter(t_abs_all[:, 0], t_abs_all[:, 2], t_abs_all[:, 1], color=[(0, 1, 0)])\n",
    "\n",
    "maximum = None\n",
    "minimum = None\n",
    "\n",
    "if KITTI == True:\n",
    "    t_abs_gt = np.array(t_abs_gt)\n",
    "    ax.scatter(t_abs_gt[:, 0], t_abs_gt[:, 2], t_abs_gt[:, 1], color=(1, 0, 0))\n",
    "    minimum = np.array(np.concatenate([t_abs_all, t_abs_gt])).min()\n",
    "    maximum = np.array(np.concatenate([t_abs_all, t_abs_gt])).max()\n",
    "else:\n",
    "    minimum = np.array(t_abs_all).min()\n",
    "    maximum = np.array(t_abs_all).max()\n",
    "\n",
    "ax.set_xlim([minimum, maximum])\n",
    "ax.set_ylim([minimum, maximum])\n",
    "ax.set_zlim([minimum, maximum])\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Z')\n",
    "ax.set_zlabel('Y')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('pyvo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "35acd477cfcaaf878a1d13d5e7e6e4e68d98621cb7a48e0a88afaced6a1249b5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
